





Tech Log

2020. 8. 10.


AI에게 어떻게 음성을 가르칠까?








시작하며
인간은 귀로 듣고, 입으로 말하여 타인과 의사소통합니다. 나와 대화할 수 있는 존재를 창조하고 싶다는 바람은 많은 사람들이 오래전부터 상상하고, 소설로 쓰고, 연구해 왔습니다. 오늘날 그 바람은 음성을 듣고 정보를 이해하고, 음성을 만들어 정보를 전달하는 대화형 인공지능(Artificial Intelligence, 이하 AI)의 시대가 도래하게 됨으로써 그 결실을 맺었습니다. 기계가 사람의 음성을 듣는 음성인식(Speech recognition)은 AI의 귀이고, 기계가 사람의 음성으로 말하는 음성합성(Speech synthesis)은 AI의 입이라고 할 수 있습니다.
 
하지만 AI가 음성을 이해하고, 활용할 수 있도록 가르치는 것은 쉽지 않은 일입니다. 사람이 몇 초면 만들 수 있는 음성 한 문장에는 너무나 많은 정보가 숨겨져 있으며, 다양한 목적을 가진 AI를 만들기 위해서는 각각 그 목적에 필요한 음성 정보를 이해하도록 가르쳐야 하기 때문입니다. 따라서 음성을 다루는 AI를 만들려는 개발자는 무엇보다 그 자신이 음성에 대해 잘 알고 있어야 합니다. 이번 글에서는 사람이 말하고 듣는 음성 한 문장에 어떤 정보들이 들어가 있는지, 이것들이 어떻게 AI를 가르치는 데 사용되는지를 최대한 수식을 배제한 채 키워드 위주로 설명하고자 합니다.
 

음성의 숨은 정보를 찾아내야 하는 이유
인간은 다양한 감각 기관을 사용해 외부로부터 오감(五感)을 인지해 정보를 입력받을 수 있습니다. 그중에서도 제일 자주 사용되는 시각과 청각은 가장 효과적으로 정보를 입력받을 수 있는 감각입니다. 하지만 같은 정보를 담고 있을지라도 음성은 이미지와는 그 형태가 매우 다르며, 이를 인지하기 위한 인간의 귀도 눈과는 완전히 다른 방식으로 동작합니다.
 
가령 ‘헤이카카오’라는 정보가 담긴 이미지와 음성이 [그림 1]과 같이 주어졌다고 가정해봅시다. 왼쪽 이미지를 보면 사람은 어떤 문장이 적혀있는지를 쉽게 인지할 수 있습니다. 하지만 음성 파형은 눈으로 봐도 어느 부분이 어떤 단어를 의미하는지를 알 수 없습니다. 이는 컴퓨터의 입장에서도 마찬가지인데요. 같은 문장을 한 사람이 여러 번 발음해도 음성 파형의 값을 비교해보면 그 분포가 제각기 매우 다르기 때문입니다.
 
[그림 1] ‘헤이 카카오’가 적힌 이미지와 음성 파형


 
음성에 들어있는 정보(발음의 종류, 성별, 음색, 높이 등)는 음성 신호 자체에서 쉽사리 얻어낼 수 없고, 수학적인 신호 처리를 거쳐야만 추출할 수 있습니다. 그중 대표적인 한 가지로, 음성을 주파수(frequency, 단위: Hertz)라는 또 다른 축으로 관측하는 방법이 있는데요. 주파수란 신호가 1초에 몇 번 진동했는지를 나타내는 수치이며, 소리는 빠르게 진동할수록, 즉 주파수가 높을수록 음이 높게 들립니다. 자연에서 들을 수 있는 모든 소리는 다양한 주파수 성분들의 합으로 이루어져 있습니다.
 
푸리에 변환(Fourier transform)이라는 함수를 사용하면 특정 시간 길이의 음성 조각(이를 프레임이라고 부름)이 각각의 주파수 성분들을 얼마만큼 갖고 있는지를 의미하는 스펙트럼(spectrum)을 얻을 수 있습니다. 이렇게 음성 전체로부터 얻은 여러 개의 스펙트럼을 시간 축에 나열하면 시간 변화에 따른 스펙트럼의 변화인 스펙트로그램(spectrogram)을 얻게 됩니다. 사람의 귀 또한 이와 유사한 메커니즘을 갖고 있어 소리에 들어 있는 각각의 주파수 성분들을 추출하는 방식으로 청취한 소리에 내재된 정보들을 얻는 것입니다.
 
[그림 2] ‘이’ 발음에 해당하는 스펙트럼(왼쪽) 과 ‘헤이 카카오’ 음성에서 추출한 스펙트로그램(오른쪽)


 
사실, 음성을 처리하는 데에는 수많은 문제가 존재합니다. 이를 해결하기 위해서는 인간이 음성을 만드는 과정과 소리를 듣는 과정을 좀 더 깊게 이해할 필요가 있습니다. 이러한 과정들을 수학적으로 모델링하여 음성의 정보를 추출하거나 필요에 따라 가공할 수 있고, 또 이렇게 얻어낸 수치들인 특징 벡터(feature vector)를 통해 AI가 음성을 이해하도록 가르칠 수 있습니다.
음성이 만들어지는 과정
발음을 결정하는 소리의 최소 단위인 음소(phoneme)는 크게 2가지로 구분할 수 있는데, 발성할 때 성대(vocal cord)의 진동을 동반하는 유성음(voiced)과 진동 없이 성대를 통과하는 무성음(unvoiced)이 있습니다.(출처: 다음백과)
 
사람의 발성 구조를 공학적으로 해석할 때, 성대를 막 통과한 소리를 여기 신호(excitation signal)라고 부릅니다. 이때 유성음 여기 신호의 파형은 준주기성(quasi-periodic)을 띄게 되며, 성대의 진동 속도에 따라 고유의 기본 주파수(fundamental frequency)와 기본 주파수의 배수에 해당하는 여러 배음(harmonics)들로 전체 스펙트럼이 구성됩니다. 반면, 무성음 여기 신호의 파형은 성대가 진동하지 않아 다양한 주파수 성분이 고르게 포함된 백색소음(white noise)과 같은 스펙트럼을 갖습니다.
 
[표 1] 음소의 종류 및 여기 신호의 형태



종류


예시


여기 신호의 모델링




유성음(Voiced)


모든 모음
(/a/, /e/, /i/, …)


Quasi-periodic signal




무성음(Unvoiced)


대부분의 자음
(/p/, /t/, /k/, ...)


White noise




[그림 3]은 사람이 음성을 만들 때 사용하는 기관들의 동작과 이들을 각각 공학적으로 모델링한 특징 정보들의 관계를 나타냅니다. 처음 폐에서 만드는 압축된 공기는 백색소음에 가까운 비주기성(aperiodicity) 신호로, 정규분포와 같이 쉽게 사용할 수 있는 확률분포로 모델링할 수 있습니다. 성대를 통과한 직후의 여기 신호는 유성음/무성음 여부에 따라 구분되며, 유성음의 경우 기본 주파수 등의 특징을 담고 있습니다. 이후 목, 코, 입, 혀 등의 성도(vocal tract)를 통과하며 발음이 결정되는데, 발음마다 성도의 구조가 달라져 증폭되는 주파수 대역과 감쇠되는 대역 역시 달라지게 됩니다. 이를 스펙트럼 포락선(spectral envelope)이라고 하며, 발음의 종류를 결정하는 주요한 특징으로 꼽힙니다.
 
이렇게 각 발성 기관의 동작을 공학적으로 모델링하고, 각 특징들을 추출, 변환, 예측하는 기술이 음성압축(speech coding), 음성변환(voice conversion), 음성합성(speech synthesis) 등의 영역에 적용되어 연구 개발되어 왔습니다.
 
[그림 3] 사람의 발성 구조를 공학적으로 모델링한 그림


음성을 듣는 과정
사람이 소리를 듣는 과정을 살펴보면 다음과 같습니다. 소리를 듣는 기관인 귀는 귓바퀴에서 소리를 모으고, 고막과 이소골이 진동하여 달팽이관의 청각 세포를 자극하면 전기 신호가 발생해 이를 뇌에 전달하는 방식으로 동작합니다.(출처: 다음백과)달팽이관은 마치 길게 늘어진 관을 돌돌 말은 모양과 같으며, 액체로 가득 차 있는 이 관에는 청각 세포들이 일렬로 나열해 있는 코르티 기관이 존재합니다. 밖에서 진동이 전달되면 코르티 기관의 특정 청각 세포가 자극되어 전기 신호를 발생시키는데, 청각 세포마다 인지할 수 있는 주파수 대역이 다릅니다.
 
[그림 4]를 보면 달팽이관의 가장 안쪽 청각 세포는 저주파 대역을 인지하며, 바깥쪽 청각 세포는 고주파 대역을 인지한다는 점을 알 수 있습니다. 중요한 점은 모든 주파수 대역을 같은 비중으로 인지하지 않고, 고주파에서 저주파로 내려갈수록 담당하는 주파수 대역이 점점 더 조밀해진다는 점입니다. 실제로 고주파 대역보다는 저주파 대역에 소리의 의미 있는 정보가 집중되어 있음을 생각한다면, 이미 인간의 청각 구조는 보다 중요한 음성 정보에 더 집중해서 들을 준비가 되어있다는 점을 알 수 있습니다.
 
[그림 4] 달팽이관의 주파수 별 청각 인지 구조를 모델링한 그림 (출처: Encyclopedia Britannica)


 
멜 스케일(이하 Mel scale)은 실제 주파수 정보를 인간의 청각 구조를 반영하여 수학적으로 변환하기 위한 대표적인 방법입니다. 높이가 다른 2개의 음을 사람에게 들려줬을 때, 사람이 인지하는 차이와 두 음의 실제 주파수 차이를 다양하게 조사하여 통계가 구축되었고, 이를 대략적으로 따라가는 간단한 함수로 두 단위 간의 관계가 정의되었습니다.(출처: Wikipedia) Mel Scale은 주파수 성분을 중요도에 따라 차등적으로 사용하기 위한 좋은 지표로써 다양한 음성처리 분야에서 사용되고 있습니다.
 
[그림 5] Mel scale과 Hertz scale간의 비교 (출처: Wikipedia)


AI에게 음성을 가르치려면?
가장 간단한 예시로 목소리의 성별을 분류하는 모델을 만들고 싶다고 한다면, 목소리의 높이 정보가 담겨있는 기본 주파수를 사용하여 중간에 기준선을 긋는 것만으로도 어느 정도 분류 정확도를 얻을 수 있을 것입니다. 하지만 음성처리에는 음성인식, 음성합성, 화자인식, 분류 등 훨씬 풀기 어려운 연구 분야들이 존재하며, 이들을 해결하기 위해서는 우선 음성의 어떤 정보를 사용해야 하는지를 최우선으로 고민해야 합니다. 대표적으로 음성을 문장으로 변환하는 음성인식 모델을 만들려면 화자가 누구든 상관없이 문장을 동일하게 인식해야 하므로 기본 주파수와 같이 화자에 종속적인 정보보다는 발음 정보와 같은 것이 더 중요합니다.
 
Mel-Frequency Cepstral Coefficient(MFCC)는 음성인식 영역에서 대표적으로 사용되는 특징 벡터입니다. MFCC를 추출하는 과정은 다음과 같습니다.(출처: Wikipedia)
1.   전체 오디오 신호를 일정 간격으로 나누고 푸리에 변환을 거쳐 스펙트로그램을 구합니다.2.  각 스펙트럼의 제곱인 파워 스펙트로그램에 Mel scale filter bank를 사용해 차원 수를 줄입니다.3.  cepstral 분석을 적용해 MFCC를 구합니다.
Cepstral 분석은 푸리에 변환을 거쳤을 때 시간 축에서 천천히 변하는 정보가 낮은 주파수 성분에 위치하고, 빨리 변하는 정보가 높은 주파수 성분에 위치한다는 점에 착안한 방법입니다.(출처: Wikipedia) 주파수 축에서 다시 한번 푸리에 변환을 사용하면 천천히 변하는 스펙트럼 포락선 정보는 낮은 성분에 위치하고, 빨리 변하는 여기 신호 정보는 높은 성분에 위치하게 되어, 적절한 취사선택을 통해 원하는 정보가 내재된 특징 벡터를 만들 수 있습니다. 
 
[그림 6] Mel scale filter bank의 예시 (출처: 개인 블로그)


 
MFCC를 계산하는 과정은 다소 복잡하지만, 그만큼 효과적인 음성 정보를 추출해 낼 수 있습니다. 인간의 청각 구조를 반영한 Mel scale 기반 filter bank[그림 6]를 사용하여 효율적으로 특징을 압축할 수 있고, cepstral 분석을 통해 음성인식에 필요한 발음 특성을 스펙트럼 포락선 정보로 구할 수 있습니다.
 
이외에도 MFCC와 비슷한 접근법의 Linear Predictive Coding(LPC), Perceptual Linear Prediction(PLP) 등 다양한 방법론이 존재하고, 음성의 시간 축 변화를 좀 더 반영하기 위해 앞서 설명한 값들의 1차, 2차 미분값(delta, delta-delta)을 추가하기도 하며, 성능 향상을 위해 zero crossing rate(ZCR), energy 등을 사용하기도 합니다. 이처럼 음성 특징 벡터의 종류가 다양하므로, 학습하려는 모델에 알맞는 특징을 적절히 선택하는 것이 중요합니다.
 
최근에는 하드웨어의 발달로 연산 속도와 메모리 용량이 충분히 증가했고, Deep Neural Network(DNN)에 대한 연구가 매우 활발히 이루어지면서 복잡한 음성 도메인 지식의 요구가 최소화된 End-to-End(E2E) 방식 모델의 연구가 주목받고 있습니다. 아직 음성 파형 그대로를 입력받는 완전한 E2E 방식은 성능 및 메모리 측면에서 갈 길이 멀지만, 최소한의 특징 벡터를 사용할 수 있어 다양한 분야에서 기존의 전통적인 음성 모델들의 성능을 빠르게 추격하거나 심지어 뛰어넘고 있습니다.
 
카카오엔터프라이즈의 AI 음성처리 서비스 소개
카카오의 인공지능 플랫폼 Kakao i에는 사용자를 위한 다양한 AI 서비스가 반영되어 있습니다. 지금부터 카카오엔터프라이즈의 음성처리 기술이 탑재된 Kakao i 음성 엔진에 대해 간단히 소개하고자 합니다.
음성인식



 
음성인식은 앞서 설명한 바와 같이 입력받은 음성을 분석해 문장으로 변환하는 기술을 말합니다. 현재 카카오엔터프라이즈에서는 뉴톤(Newtone)이라는 이름으로 카카오맵, 현대자동차 등에 음성인식 서비스를 제공하고 있습니다. 음성인식에 대한 자세한 정보는 김훈님의 [카카오AI리포트]음성인식 방법과 카카오i의 음성형엔진 을 참고하시기 바랍니다.
음성합성



음성합성이란, 입력받은 문장으로부터 사람의 목소리를 생성하는 기술입니다. 수많은 음성 단위들을 연결하여 완성된 문장을 만드는 Unit selection 방식의 음성합성 모델이 뉴톤 톡(Newtone Talk)이라는 이름으로 카카오미니, 카카오내비, 카카오버스, 카카오지하철 등 다양한 서비스에 적용되어 있습니다.
 
최근 DNN 방식의 음성합성 엔진인 딥 보이스(Deep Voice)가 카카오브레인과의 협업으로 개발되어 다양한 기기 속 뉴스 읽기 서비스에 활용되고 있습니다. 관련 정보는 [후기] 딥보이스 제작 비하인드 스토리 에서 더 자세하게 확인할 수 있습니다.
 
카카오엔터프라이즈의 음성인식과 음성합성 기술은 Open API를 통해 누구든지 서비스를 사용할 수 있으며, Kakao Developers 에서 확인해 볼 수 있습니다.
핵심어 검출



핵심어 검출은 사용자가 kakao i 디바이스를 깨우기 위해 사용되는 기술입니다. ‘헤이카카오’나 ‘카카오미니’ 등의 이름을 부르면 kakao i와 대화를 시작할 수 있습니다. 현재는 지정되어 있는 단어들만 인식이 가능하지만 사용자가 지정한 단어로 kakao i가 적용된 기기들을 깨우기 위한 연구가 진행되고 있습니다. 핵심어 검출에 대한 자세한 정보는 정대성님과 박종세님의 "헤이, 카카오!"를 불러야 하는 이유 를 참고하기 바랍니다.
화자 인식



화자 인식은 입력된 음성을 미리 등록된 DB와 비교하여 화자가 누구인지 식별하는 기술로, 카카오미니 역시 이를 통해 대화를 시작한 사람이 누구인지 알아낼 수 있습니다. 현재, 등록된 목소리와 일치하는 목소리로 카카오미니를 불렀을 때, 사용자에게 맞춤형 답변을 제공하는 보이스 프로필이 베타서비스 중에 있습니다. 화자 인식 관련 자세한 내용은 [카카오AI리포트]카카오미니는 목소리를 어떻게 인식할까 를 참고하기 바랍니다.
 

마치며
지금까지 사람의 말소리 한 문장에 어떤 음성 정보들이 들어 있는지, 그리고 이를 AI가 이해할 수 있는 특징 벡터로 어떻게 만드는지 등에 대한 기본적인 내용을 설명했습니다. 카카오엔터프라이즈는 나아가 사용자에게 새롭고 더 편리한 경험을 선사하기 위해, 앞서 설명하지 않은 다양한 연구 결과를 축적 중이며 관련된 서비스를 제공할 계획입니다.
 
또한, 앞으로 연재될 블로그 글과 if kakao 행사를 통해 더 발전된 카카오엔터프라이즈의 음성처리 연구들을 여러분들에게 소개하겠습니다.
 


새로운 길에 도전하는 최고의 Krew들과 함께 해요!
[AI기술] 음성처리 기술 개발자 모집





장원(taylor)
보다 선명하고, 보다 인간적인 감정을 녹여낼 수 있는 미래 음성합성 기술을 연구하는 중입니다.일상의 사소한 불편을 해소하기 위한 kakao i의 목표에 기여하고 싶습니다.


 




박진우(woody)
만화를 보며 로봇과 대화하는 세상을 꿈꿨던 개발자입니다.꿈꾸던 미래를 위해 다양한 플랫폼에 카카오엔터프라이즈의 음성처리 기술을 연결 시키는 일을 하고 있습니다.







공유하기

게시글 관리


구독하기카카오엔터프라이즈 기술블로그 Tech&(테크앤)


저작자표시 비영리 변경금지






Tag
Kakao I, Mel scale, MFCC, 음성, 음성인식, 음성합성, 카카오미니, 특징벡터, 핵심어검출, 화자인식


관련글






스마트하게 식단을 관리하는 딥러닝 기술







카카오 i의 작고 소중한 힐링







정답 유형을 분류하는 딥러닝 기술







카카오 i, 누구냐 넌





댓글0










